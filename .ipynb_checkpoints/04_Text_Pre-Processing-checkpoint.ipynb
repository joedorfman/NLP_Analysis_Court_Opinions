{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import data\n",
    "from gensim.models import word2vec\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of all the appellate cases from the 9th circuit\n",
    "\n",
    "df=pd.read_pickle('./circuit_w_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This the list of all the appellate cases except the 9th circuit\n",
    "\n",
    "df2=pd.read_pickle('./circuit_dc_w_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 40)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 40)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop out entries that are blank or NAN in the text field.\n",
    "\n",
    "df=df.dropna(subset=['text'])\n",
    "\n",
    "df=df.loc[df['text']!='']\n",
    "\n",
    "df2=df2.dropna(subset=['text'])\n",
    "\n",
    "df2=df2.loc[df2['text']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 40)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Matt Brems' lecture notes\n",
    "\n",
    "def review_to_wordlist(sentence):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #    \n",
    "    \n",
    "    # tokenizing and removing punctuation\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]{3,}')\n",
    "    text_processed=tokenizer.tokenize(sentence)\n",
    "    \n",
    "    legal_words=set(['sec', 'fed', 'reg', 'act', 'cir', 'cert', 'see', 'app', 'soc', 'stat'])\n",
    "    stops=set(stopwords.words('english'))\n",
    "    \n",
    "    # removing any stopwords\n",
    "    text_processed = [word for word in text_processed  if (word.lower() not in stops and \n",
    "                     word.lower() not in legal_words)]\n",
    "          \n",
    "    # make words lower-cased, unless it is an acronym\n",
    "    text_processed = [word.lower() if (word.upper() != word or len(word)>4) else word for word in text_processed ]\n",
    "    \n",
    "    # return a list of words\n",
    "    return (text_processed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Matt Brems' lecture notes.\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( text, tokenizer):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the document into sentences\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n"
     ]
    }
   ],
   "source": [
    "#Break each case down into a series of sentances, each of which is a list of words\n",
    "# Assemble a list of all such sentences in the entire corpus\n",
    "\n",
    "\n",
    "train_sentences = []  # Initialize an empty list of sentences\n",
    "counter=0\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for case_text in df['text']:\n",
    "    print (counter)\n",
    "    train_sentences += review_to_sentences(case_text, tokenizer)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109863"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12 12:04:15,825 : INFO : collecting all words and their counts\n",
      "2018-07-12 12:04:15,826 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-12 12:04:15,860 : INFO : PROGRESS: at sentence #10000, processed 98026 words, keeping 7796 word types\n",
      "2018-07-12 12:04:15,896 : INFO : PROGRESS: at sentence #20000, processed 194672 words, keeping 11582 word types\n",
      "2018-07-12 12:04:15,930 : INFO : PROGRESS: at sentence #30000, processed 293803 words, keeping 13854 word types\n",
      "2018-07-12 12:04:15,966 : INFO : PROGRESS: at sentence #40000, processed 395471 words, keeping 16096 word types\n",
      "2018-07-12 12:04:16,000 : INFO : PROGRESS: at sentence #50000, processed 497056 words, keeping 17779 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12 12:04:16,034 : INFO : PROGRESS: at sentence #60000, processed 600534 words, keeping 18974 word types\n",
      "2018-07-12 12:04:16,080 : INFO : PROGRESS: at sentence #70000, processed 699081 words, keeping 19963 word types\n",
      "2018-07-12 12:04:16,109 : INFO : PROGRESS: at sentence #80000, processed 773462 words, keeping 20600 word types\n",
      "2018-07-12 12:04:16,146 : INFO : PROGRESS: at sentence #90000, processed 879668 words, keeping 22230 word types\n",
      "2018-07-12 12:04:16,185 : INFO : PROGRESS: at sentence #100000, processed 980906 words, keeping 23205 word types\n",
      "2018-07-12 12:04:16,219 : INFO : collected 24830 word types from a corpus of 1080367 raw words and 109863 sentences\n",
      "2018-07-12 12:04:16,220 : INFO : Loading a fresh vocabulary\n",
      "2018-07-12 12:04:16,341 : INFO : min_count=40 retains 3619 unique words (14% of original 24830, drops 21211)\n",
      "2018-07-12 12:04:16,342 : INFO : min_count=40 leaves 948364 word corpus (87% of original 1080367, drops 132003)\n",
      "2018-07-12 12:04:16,357 : INFO : deleting the raw counts dictionary of 24830 items\n",
      "2018-07-12 12:04:16,360 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2018-07-12 12:04:16,362 : INFO : downsampling leaves estimated 905947 word corpus (95.5% of prior 948364)\n",
      "2018-07-12 12:04:16,381 : INFO : estimated required memory for 3619 words and 100 dimensions: 4704700 bytes\n",
      "2018-07-12 12:04:16,383 : INFO : resetting layer weights\n",
      "2018-07-12 12:04:16,468 : INFO : training model with 4 workers on 3619 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=40\n",
      "2018-07-12 12:04:17,479 : INFO : EPOCH 1 - PROGRESS: at 81.87% examples, 740878 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-12 12:04:17,645 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 12:04:17,656 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 12:04:17,658 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 12:04:17,664 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 12:04:17,666 : INFO : EPOCH - 1 : training on 1080367 raw words (906140 effective words) took 1.2s, 762834 effective words/s\n",
      "2018-07-12 12:04:18,684 : INFO : EPOCH 2 - PROGRESS: at 89.89% examples, 812807 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 12:04:18,781 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 12:04:18,785 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 12:04:18,792 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 12:04:18,805 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 12:04:18,806 : INFO : EPOCH - 2 : training on 1080367 raw words (905952 effective words) took 1.1s, 804504 effective words/s\n",
      "2018-07-12 12:04:19,834 : INFO : EPOCH 3 - PROGRESS: at 77.65% examples, 687012 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 12:04:20,155 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 12:04:20,157 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 12:04:20,164 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 12:04:20,173 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 12:04:20,174 : INFO : EPOCH - 3 : training on 1080367 raw words (905837 effective words) took 1.4s, 667217 effective words/s\n",
      "2018-07-12 12:04:21,194 : INFO : EPOCH 4 - PROGRESS: at 81.86% examples, 737101 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-12 12:04:21,365 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 12:04:21,374 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 12:04:21,381 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 12:04:21,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 12:04:21,385 : INFO : EPOCH - 4 : training on 1080367 raw words (906011 effective words) took 1.2s, 757057 effective words/s\n",
      "2018-07-12 12:04:22,408 : INFO : EPOCH 5 - PROGRESS: at 74.25% examples, 658177 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 12:04:22,806 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 12:04:22,808 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 12:04:22,818 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 12:04:22,821 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 12:04:22,822 : INFO : EPOCH - 5 : training on 1080367 raw words (905728 effective words) took 1.4s, 635544 effective words/s\n",
      "2018-07-12 12:04:22,823 : INFO : training on a 5401835 raw words (4529668 effective words) took 6.4s, 712910 effective words/s\n",
      "2018-07-12 12:04:22,824 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-07-12 12:04:22,872 : INFO : saving Word2Vec object under 100features_40minwords_40context, separately None\n",
      "2018-07-12 12:04:22,873 : INFO : not storing attribute vectors_norm\n",
      "2018-07-12 12:04:22,875 : INFO : not storing attribute cum_table\n",
      "2018-07-12 12:04:22,940 : INFO : saved 100features_40minwords_40context\n"
     ]
    }
   ],
   "source": [
    "# Build a Word2Vec Model from the texts of all the cases\n",
    "\n",
    "\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 40         # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(train_sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"100features_40minwords_40context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [00:02<00:00, 111.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process each text field into a list of tokens\n",
    "\n",
    "df['text2']=df['text'].progress_apply(review_to_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:01<00:00, 110.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process each text field into a list of tokens\n",
    "\n",
    "df2['text2']=df2['text'].progress_apply(review_to_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./processed_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_pickle('./dc_processed_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('chinook', 0.8492171168327332),\n",
       " ('coho', 0.795222818851471),\n",
       " ('steelhead', 0.795101523399353),\n",
       " ('soncc', 0.7336384057998657),\n",
       " ('snake', 0.7307172417640686),\n",
       " ('spawning', 0.7187743186950684),\n",
       " ('naturally', 0.706528902053833),\n",
       " ('rivers', 0.7061449885368347),\n",
       " ('klamath', 0.7029423713684082),\n",
       " ('hatchery', 0.6934346556663513)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('salmon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bringing', 0.7627053260803223),\n",
       " ('defendant', 0.7516956329345703),\n",
       " ('redressability', 0.7488840818405151),\n",
       " ('injury', 0.7443935871124268),\n",
       " ('concrete', 0.7401298880577087),\n",
       " ('establish', 0.7238608598709106),\n",
       " ('suffered', 0.7148732542991638),\n",
       " ('asserting', 0.7141393423080444),\n",
       " ('redressed', 0.7087077498435974),\n",
       " ('interests', 0.6911041736602783)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('plaintiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('violations', 0.7980332970619202),\n",
       " ('alleged', 0.7750270962715149),\n",
       " ('violating', 0.7222898006439209),\n",
       " ('allege', 0.6760566830635071),\n",
       " ('knowing', 0.6743615865707397),\n",
       " ('unlawful', 0.6540881395339966),\n",
       " ('alleges', 0.653792679309845),\n",
       " ('liability', 0.6328064203262329),\n",
       " ('bring', 0.6232904195785522),\n",
       " ('violate', 0.6145861148834229)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('violation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('capture', 0.9548852443695068),\n",
       " ('hunt', 0.9316563606262207),\n",
       " ('harass', 0.9220545291900635),\n",
       " ('trap', 0.9220305681228638),\n",
       " ('wound', 0.9114148616790771),\n",
       " ('shoot', 0.891943097114563),\n",
       " ('porpoises', 0.8889783024787903),\n",
       " ('collect', 0.8788501024246216),\n",
       " ('migratory', 0.878006637096405),\n",
       " ('harassment', 0.8670342564582825)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('kill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
